[
  {
    "objectID": "contents.html",
    "href": "contents.html",
    "title": "Contents",
    "section": "",
    "text": "Note\n\n\n\nFor the past several decades, the primary languge used in the lab was MATLAB. Starting from 2021, there’s been a combined effort to use Julia for new projects in the lab - for it’s promised speed and intuitive mathematical-style language syntax. However, Julia being a new language, the ecosystem lacks any equivalent for many of the mature Psychometrics packages available in R. Also, most of PI’s previous software are written in MATLAB. Hence, for the time being we use MATLAB, R, and Julia depending on the needs of the project, but the plan is to completely move to Julia in the future.\n\n\n\n\n\nGetting Started in Julia\nSetting up Julia via juliaup\n\n\n\n\n\nLinear Algebra: Vectors and Matrices (03/09/2023)\n\nLinear Algebra: Application using SVD (03/09/2023)\nLinear Algebra - SVD (10/10/2022)\nCognitive Modeling Lab - Intro\n\n\n\n\n\nList of R Packages used in the Lab"
  },
  {
    "objectID": "contents.html#books",
    "href": "contents.html#books",
    "title": "Contents",
    "section": "Books",
    "text": "Books\n\nBayesian Inference\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman and Hall/CRC."
  },
  {
    "objectID": "contents.html#bayesian-networks",
    "href": "contents.html#bayesian-networks",
    "title": "Contents",
    "section": "Bayesian Networks",
    "text": "Bayesian Networks\n\nAlmond, R. G., Mislevy, R. J., Steinberg, L. S., Yan, D., & Williamson, D. M. (2015). Bayesian networks in educational assessment. Springer.\nScutari, M., & Denis, J. B. (2021). Bayesian networks: with examples in R. Chapman and Hall/CRC."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COINS Lab Wiki",
    "section": "",
    "text": "Welcome to COINS Lab Wiki. COINS Lab Wiki was setup to consolidate all the tutorials, tools, and resources, that would be helpful for both current & future members and collaborators, at one place.\nThis wiki was initially set up on February 18, 2022 by Athul Sudheesh using Franklin.jl (A Julia based static site generator). This was later moved to Quarto (A more advanced and easier open-source scientific & publishing system) on July 14, 2022.\n\nContributing to this Wiki\nIf you want to direclty contribute to this wiki, you need to familiaze yourself with how to use the Quarto publishing system and the synatx for makrdown files. The documentation for Quarto can be found here and markdown syntax here.\n\n\nWorking with Github\n\nFirst time users\n\nSetup your GitHub account (if you already haven’t) and share your GitHub ID with the Principal Investigator or one of the senior PhD students in the lab. Inform them of your interest in contributing to the lab wiki. Either the PI or one of the senior PhD students will add you to the wiki repository as a contributor.\nInstall Github Desktop and link it with your Github profile.\nGo to https://github.com/coinslab/coinslab.github.io and click on Code -> Open with Github Desktop\n\n\nThis should create a clone of the Knowledge-Base repository in your local machine.\n\nNow you can go to the file location in your computer where you have cloned the coinslab.github.io repository.\nYou can create contents for this site as a Quarto Markdown file(.qmd) inside the pages folder in coinslab.github.io. You can use VS Code to create and edit the .qmd files.\nOnce you are done with writing/updating your section, you can commit your changes and push the changes to the Github using your Github Desktop client. Now inform the Senior Investigator who is in charge of this site. He/She will build and publish the updates\n\n\n\nExperienced users\n\nIf you are not contributing for the first time, always fetch from the cloud and make sure everything is in-sync with the cloud, before beginning to work on the documentation."
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#before-we-start",
    "href": "pages/CLA/slides/Lec1.html#before-we-start",
    "title": "Computational Linear Algebra",
    "section": "Before we start…",
    "text": "Before we start…\n\nCreate dedicated project environments for different projects\nActivate your project environment before you start working\n\n\nusing Pkg\nPkg.activate(\".\")\n\n\nTo add a package:\n\n\nusing Pkg\nPkg.add(\"PkgName\")"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#matrices-are-everywhere",
    "href": "pages/CLA/slides/Lec1.html#matrices-are-everywhere",
    "title": "Computational Linear Algebra",
    "section": "Matrices are everywhere",
    "text": "Matrices are everywhere\n\n\n\nTabular Data\n\n\n\n\nImages\n\n\n\n\n\nGraph Models\n\n\n\n\n\n\nMarkov Chains"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#images-as-matrices",
    "href": "pages/CLA/slides/Lec1.html#images-as-matrices",
    "title": "Computational Linear Algebra",
    "section": "Images as Matrices",
    "text": "Images as Matrices\nLoading Images\n\n# Packages for handling images \nusing Images, ImageIO, ImageMagick \n# First time installation of Images package \n# could take couple of minutes.\n\n\nDefining path to files\n\n\ncat = joinpath(pwd(),\"images\", \"cat.1.jpg\")\n\n\nLoading the image\n\n\ncatimg = load(cat)"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#images-as-matrices-1",
    "href": "pages/CLA/slides/Lec1.html#images-as-matrices-1",
    "title": "Computational Linear Algebra",
    "section": "Images as Matrices",
    "text": "Images as Matrices\nImages are Multidimensional Matrices\n\n[Height x Width] x Color\n\n\nm,n = size(catimg)\n\n(280, 300)\n\n\n\nr = RGB.(red.(catimg), zeros(m,n), zeros(m,n))\ng = RGB.(zeros(m,n), green.(catimg), zeros(m,n))\nb = RGB.(zeros(m,n), zeros(m,n), blue.(catimg))"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#images-as-matrices-2",
    "href": "pages/CLA/slides/Lec1.html#images-as-matrices-2",
    "title": "Computational Linear Algebra",
    "section": "Images as Matrices",
    "text": "Images as Matrices\nImages are Multidimensional Matrices\n\nr .+ b .+ g \n\n\n\n\n\nMany a times we do machine learning after turning images into GrayScale\n\n\nGray.(catimg)"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#images-as-matrices-3",
    "href": "pages/CLA/slides/Lec1.html#images-as-matrices-3",
    "title": "Computational Linear Algebra",
    "section": "Images as Matrices",
    "text": "Images as Matrices\nImages are Multidimensional Matrices\n\nFloat64.(Gray.(catimg))\n\n280×300 Matrix{Float64}:\n 0.164706   0.164706   0.168627   0.172549   …  0.788235  0.768627  0.764706\n 0.168627   0.168627   0.168627   0.172549      0.772549  0.756863  0.74902\n 0.168627   0.168627   0.168627   0.168627      0.756863  0.752941  0.752941\n 0.164706   0.160784   0.160784   0.156863      0.74902   0.760784  0.776471\n 0.160784   0.156863   0.14902    0.145098      0.737255  0.760784  0.780392\n 0.160784   0.156863   0.14902    0.141176   …  0.733333  0.752941  0.764706\n 0.172549   0.164706   0.152941   0.145098      0.745098  0.756863  0.760784\n 0.180392   0.172549   0.160784   0.14902       0.760784  0.772549  0.772549\n 0.188235   0.176471   0.172549   0.172549      0.807843  0.803922  0.784314\n 0.176471   0.168627   0.164706   0.168627      0.768627  0.756863  0.733333\n 0.164706   0.156863   0.156863   0.164706   …  0.776471  0.756863  0.72549\n 0.152941   0.145098   0.152941   0.160784      0.796078  0.776471  0.745098\n 0.145098   0.141176   0.152941   0.164706      0.772549  0.760784  0.737255\n ⋮                                           ⋱                      \n 0.105882   0.101961   0.0941176  0.0901961     0.188235  0.192157  0.207843\n 0.0901961  0.0901961  0.0862745  0.0823529     0.203922  0.196078  0.192157\n 0.0823529  0.0823529  0.0784314  0.0784314  …  0.282353  0.258824  0.247059\n 0.0784314  0.0784314  0.0784314  0.0784314     0.360784  0.333333  0.317647\n 0.0823529  0.0823529  0.0823529  0.0784314     0.243137  0.184314  0.211765\n 0.0823529  0.0784314  0.0784314  0.0745098     0.215686  0.133333  0.137255\n 0.0823529  0.0823529  0.0745098  0.0705882     0.188235  0.109804  0.109804\n 0.0941176  0.0901961  0.0823529  0.0745098  …  0.160784  0.117647  0.137255\n 0.109804   0.101961   0.0941176  0.0823529     0.121569  0.117647  0.160784\n 0.121569   0.113725   0.0980392  0.0862745     0.101961  0.129412  0.192157\n 0.121569   0.113725   0.0980392  0.0823529     0.12549   0.141176  0.184314\n 0.121569   0.109804   0.0941176  0.0784314     0.160784  0.133333  0.12549"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#image-compression-using-svd",
    "href": "pages/CLA/slides/Lec1.html#image-compression-using-svd",
    "title": "Computational Linear Algebra",
    "section": "Image compression using SVD",
    "text": "Image compression using SVD\n\nusing LinearAlgebra \nU,S,V = svd(Float64.(Gray.(catimg)));\n\n\nReconstructing the image using the singular values\n\n\nRGB.(U*diagm(S)*V')"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#image-compression-using-svd-1",
    "href": "pages/CLA/slides/Lec1.html#image-compression-using-svd-1",
    "title": "Computational Linear Algebra",
    "section": "Image compression using SVD",
    "text": "Image compression using SVD\n\nfunction compressimg(n)\n    RGB.(U[:,1:n]*diagm(S[1:n])*V[:,1:n]')\nend\n\ncompressimg (generic function with 1 method)\n\n\n\ncompressimg(10)\n\n\nExample 4.2.3 (page 110)"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#whats-the-optimal-number-of-features",
    "href": "pages/CLA/slides/Lec1.html#whats-the-optimal-number-of-features",
    "title": "Computational Linear Algebra",
    "section": "What’s the optimal number of features?",
    "text": "What’s the optimal number of features?\n\nusing GRUtils \nSNorm = S ./norm(S)\nplot(1:length(S),SNorm, title=\"Scree Plot\", \n        xlabel = \"Singular Value IDs\",\n        ylabel = \"Singular Values\", grid=false)"
  },
  {
    "objectID": "pages/CLA/slides/Lec1.html#svd-as-recoding-strategy-image-example",
    "href": "pages/CLA/slides/Lec1.html#svd-as-recoding-strategy-image-example",
    "title": "Computational Linear Algebra",
    "section": "SVD as recoding strategy (Image Example)",
    "text": "SVD as recoding strategy (Image Example)\n\nusing Glob # For reading multiple files in a folder \nusing Pipe: @pipe\nfunction recodeimage(pathtoimage)\n    # The following code block looks for .jpg/.JPG/.png/.PNG files and create a list of them \n    imagelist = glob(\"*.jpg\", pathtoimage)  \n    append!(imagelist,  glob(\"*.JPG\", pathtoimage))\n    append!(imagelist,  glob(\"*.png\", pathtoimage)) \n    append!(imagelist,  glob(\"*.PNG\", pathtoimage))\n\n\n    # Algorithm for scree plot ===================================================================================\n    # creating a temporary array of arrays to hold the numerical values of the grayscaled images \n    # temp[1] will have the grayscaled information of image 1, and so on...\n    temp = Array{Array{Float64,2},1}(undef,length(imagelist))\n    \n    # @showprogress is a macro to print the progress of this loop when this function is run \n        for i in 1:length(imagelist)\n        temp[i] = Float64.(Gray.(imresize(load(imagelist[i]),128,128))) \n        # @pipe is a macro for chaining multiple tasks\n        # the next two blocks of code takes the image, converts it into grayscale and compute \n        # the singular values, then only the first n_singular values are stored in the recodedArray    \n        #img_singluar = @pipe X |> Float64.(Gray.(_)) |> svdvals(_)[1:n_singularvlas]' \n        #recodedArray = vcat(recodedArray, img_singluar) \n    end\n    \n    # stacking individual images to create a giant image matrix \n    stackedX = vcat(temp...)\n    println(\"Running Singular Value Decomposition...\")\n    U,S,V= svd(stackedX)\n    S = S ./norm(S)\n    screeplot(S)\n    # ============================================================================================================\n    \n    # After examining the scree plot, the user decides the no. of singular values \n    n_singularvlas = input(\"No. of Features (due to bug in the code that reads user inputs, you might have to enter the no twice, if the program didn't run first time)\")\n    \n    # Initializing an empty array to store the n_singular values of the images \n    recodedArray = Array{Float64}(undef, 0, n_singularvlas) \n    # computing singular values using only the first n_singulars \n    for imagearrays in temp\n        img_singular = @pipe imagearrays |> svdvals(_)[1:n_singularvlas]'\n        recodedArray = vcat(recodedArray, img_singular)\n    end\n\n    # Normalizing the singular values \n    recodedArray =  eachcol(recodedArray) ./ norm.(eachcol(recodedArray))\n    # writing the array as a .csv file \n    filename = joinpath(pathtoimage, \"image_recoded.csv\")\n    CSV.write(filename,  DataFrame(recodedArray, :auto), writeheader=true)\nend\n\nrecodeimage (generic function with 1 method)"
  },
  {
    "objectID": "pages/julia-getting-started.html",
    "href": "pages/julia-getting-started.html",
    "title": "Getting Started in Julia",
    "section": "",
    "text": "(If you are new to programming)\n\nJulia Installation & Setup (Part of Introduction to Julia Machine Learning book)\n\n\n\n\n(If you are proficient in Python/MATLAB/R)\n\n[Cheatsheet] Comparison of basic statistics/linear algebra in Python/Julia/MATLAB\n[Official Documentation] Julia’s noteworthy differences from other languages\n[MIT Opencourseware] Introduction to Computational Thinking\n[Online Book & Lectures] Introduction to Parallel Computing and Scientific Machine Learning\n\n\n\n\n(If you have developed APIs and software in other languages)\n\n[Book] Kwong, T. (2020). Hands-On Design Patterns and Best Practices with Julia: Proven solutions to common problems in software design for Julia 1. x. Packt Publishing Ltd.\n[Book] Balbaert, I., Sengupta, A., & Sherrington, M. (2016). Julia: High Performance Programming. Packt Publishing Ltd."
  },
  {
    "objectID": "pages/julia-getting-started.html#packages-for-computational-modeling",
    "href": "pages/julia-getting-started.html#packages-for-computational-modeling",
    "title": "Getting Started in Julia",
    "section": "Packages for Computational Modeling",
    "text": "Packages for Computational Modeling\nYou can search through the list of all registered julia package here.\n\nList of packages for basic modeling (Linear Models, Clustering, Hypothesis Testing, etc.)\nMainstream Machine Learning\n\nScikitLearn.jl\n\n[Learning Resource] Sudheesh, A. (2022). Introduction to Julia Machine Learning\n\nMLJ.jl\n\nNeural Networks\n\nFlux.jl\n\nList of packages for all sorts of optimization\nDifferential Equations based Modeling\n\nDynamicalSystems.jl\nDiffEq.jl\n\nBayesian Inference & Probabilistic Programming\n\nTuring.jl\n\nProbablistic Graphical Models\n\nBayesNet.jl\nJunctionTrees.jl\nMarkov Decision Process POMDPs.jl"
  },
  {
    "objectID": "pages/juliaup-setup.html",
    "href": "pages/juliaup-setup.html",
    "title": "Julia Setup via juliaup",
    "section": "",
    "text": "Go to Microsoft App store and Install Julia App.\nDownload and Install VS Code.\nInstall the Julia VS Code Extension. \nAfter installation, the “install button” will turn into a gear icon. Once you click on the gear icon, a dropdown menu will popup. From that list, choose Extension Settings.\nWithout unselecting the text on the extension search bar, type exe. Make sure the Julia: Executable Path field is completely blank. \nTo start a Julia session inside VS Code, press Ctrl + Shift + P. From the dropdown list, choose Julia: Start REPL\nIf you want to start Julia REPL without VS Code, you go to your command line and type julia and hit enter.\n\n\n\nJuliaup is a Julia version manager and is automatically installed when you install the Julia App from Microsoft App Store. Juliaup lets you add, update, and remove different Julia versions very easily.\nTo use juliaup, you open your command line and type juliaup and the command you want to run.\n\njuliaup status prints all the julia version installed in your PC.\njuliaup add release installed the latest stable version of Julia.\njuliaup add 1.8.3 will install Julia 1.8.3.\n\nTo see the list of available julia version, run juliaup list\n\njuliaup default 1.8.3 sets the default version of Julia in your PC as Julia 1.8.3. This also defines what version of Julia will be called while you start a Julia session in VS Code.\njuliaup remove 1.8.3 will remove Julia 1.8.3 from your PC."
  },
  {
    "objectID": "pages/juliaup-setup.html#mac-user",
    "href": "pages/juliaup-setup.html#mac-user",
    "title": "Julia Setup via juliaup",
    "section": "2 Mac User",
    "text": "2 Mac User\n\nInstall homebrew package manager for Mac\n\nTo install homebrew, you can copy-past and run the below code on your terminal:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Juliaup via homebrew\n\nRun the following code in your terminal:\nbrew install juliaup\n\nFollow instructions in Section 1.1 to install/remove a particular version of Julia."
  },
  {
    "objectID": "pages/r-packagelist.html",
    "href": "pages/r-packagelist.html",
    "title": "R Packages & Related Learning Resources",
    "section": "",
    "text": "An extensive list of R software for psychometric modeling can be found here.\n\nCognitive Diagnsotic Modeling\n\nCDM [Package Home Page]\n\n[Tutorial Paper] Ravand, H., & Robitzsch, A. (2015). Cognitive diagnostic modeling using R. Practical Assessment, Research, and Evaluation, 20(1), 11.\n[Tutorial Paper] George, A. C., & Robitzsch, A. (2015). Cognitive diagnosis models in R: A didactic. The Quantitative Methods for Psychology, 11(3), 189-205.\n\nGDINA [Package Home Page]\n\n[Tutorial Paper] Ma, W., & de la Torre, J. (2020). GDINA: An R package for cognitive diagnosis modeling. Journal of Statistical Software, 93, 1-26.\n\n\n[A Comparison of Software Packages Available for DINA] Sen, S., & Terzi, R. (2020). A comparison of software packages available for DINA model estimation. Applied Psychological Measurement, 44(2), 150-164.\n[Tutorial Paper on implementing CDMs using probablisitic programming in R] Zhan, P., Jiao, H., Man, K., & Wang, L. (2019). Using JAGS for Bayesian cognitive diagnosis modeling: A tutorial. Journal of Educational and Behavioral Statistics, 44(4), 473-503.\n\n\nBayesian Networks in Educational Assessment.\nA general documentation on bayesian networks in educational assessment can be found here.\n\nbnlearn [Package Home Page]\nCPTtools [Package Home Page]\nbayesvl for parameterized bayesian networks\n\n\nBayesian Networks Book Recommendations\n\nScutari, M., & Denis, J. B. (2021). Bayesian networks: with examples in R. Chapman and Hall/CRC.\nAlmond, R. G., Mislevy, R. J., Steinberg, L. S., Yan, D., & Williamson, D. M. (2015). Bayesian networks in educational assessment. Springer.\n\n\n\n\nBayesian Inference & Probablisitic Programming\n\nCmdStanR\nbayesvl\n\n\nBayesian Inference Book Recommendations\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995). Bayesian data analysis. Chapman and Hall/CRC."
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#student-objectives",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#student-objectives",
    "title": "Lab 1: Hello Julia! ",
    "section": "Student Objectives: ",
    "text": "Student Objectives: \n\nIdentify an engineering or scientific problem in Artificial Intelligence, Psychology, or Cognitive Neuroscience.\nIdentify & assess the quality of the data required to empirically investigate the problem.\nDevelop a 1 page project proposal abstract.\nUse the machine learning skills acquired through the lab and theory to investigate the problem.\n\nRecode the data using unsupervised learning (if necessary).\nImplement a simple logistic regression model, shallow neural network, and a deep neural network model.\nEvaluate the performance of your model (Confusion Matrix).\nDo 2-fold cross-validation.\nCompare the results of competing models.\n\nPresent the results of your study as a 15-minute talk.\nWrite and submit a ~10 page project report in APA Journal Article format."
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#so-1-identifying-the-problem",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#so-1-identifying-the-problem",
    "title": "Lab 1: Hello Julia! ",
    "section": "SO 1: Identifying the problem",
    "text": "SO 1: Identifying the problem\n\nYou fill out a project interest survey\n\nArtificial Intelligence Project Examples:\n\nEmail Spam Filter, Speech Recognition System, Twitter Hate Speech Detection\n\nPsychology Project Examples:\n\nHuman Emotion Classification using Eye-Feature, Psychopathy Diagnosis\n\nCognitive Neuroscience Project Examples:\n\nAlzheimer’s detection using fMRI, Analysis and Modeling of Neural Ensemble Rehearsal During Sleep, Tumor detection using MRI, Autism diagnosis using fMRI\n\n\nYou form groups by yourself or we help you find project groups"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#so-2-identify-assess-the-quality-of-the-data",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#so-2-identify-assess-the-quality-of-the-data",
    "title": "Lab 1: Hello Julia! ",
    "section": "SO 2: Identify & assess the quality of the data",
    "text": "SO 2: Identify & assess the quality of the data\nSome places to look for Data\n\nKaggle: For all sorts of datasets.\nUCI Machine Learning Repository: For all sorts of datasets.\nUCLA Library: For Psychological datasets.\nCMU Datashop: For Educational & Psychological datasets.\nOpenNeuro: For Neuroscience datasets.\nCRCNS: For Neuroscience datasets.\nRDatasets: A collection of common datasets used in all Statistical/Machine Learning textbooks.\nGoogle Dataset Search"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#so-3-project-proposal-abstract",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#so-3-project-proposal-abstract",
    "title": "Lab 1: Hello Julia! ",
    "section": "SO 3: Project Proposal Abstract",
    "text": "SO 3: Project Proposal Abstract"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#so-4-machine-learning-skillstools",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#so-4-machine-learning-skillstools",
    "title": "Lab 1: Hello Julia! ",
    "section": "SO 4: Machine Learning Skills/Tools",
    "text": "SO 4: Machine Learning Skills/Tools"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#common-ml-languages",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#common-ml-languages",
    "title": "Lab 1: Hello Julia! ",
    "section": "Common ML Languages",
    "text": "Common ML Languages"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#why-julia",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#why-julia",
    "title": "Lab 1: Hello Julia! ",
    "section": "Why Julia ?",
    "text": "Why Julia ?\n\nOpen Source (Free to Use, No license required to do commercial project in Julia)\nLess verbose (compared to R & Python)\nBest-in-class interoperability (with R, Python & C)\nReally fast when implementing computationally complex algorithms\nGood software engineering principles instantiated in the language design\nUsed widely in High Performance Scientific Computing\n\nShortcomings\n\nRelatively young community –> fewer packages compared to R or Python (RCall.jl & PythonCall.jl to the rescue)\nPackage installation can take some time (continually improving with each update)"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html#topics-to-cover",
    "href": "pages/CNMLabSlides/Lab1/lab1.html#topics-to-cover",
    "title": "Lab 1: Hello Julia! ",
    "section": "Topics to cover",
    "text": "Topics to cover\n\nInstalling Julia\nChange language type in a new file\nKnow your IDE\nJulia REPL Modes\nInstalling Packages\nCreating a Julia Script\nProject Environments"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#a",
    "href": "pages/CNMLabSlides/Intro.html#a",
    "title": "",
    "section": "A",
    "text": "A"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#why",
    "href": "pages/CNMLabSlides/Intro.html#why",
    "title": "",
    "section": "Why",
    "text": "Why"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#why-study-linear-algebra",
    "href": "pages/CNMLabSlides/Intro.html#why-study-linear-algebra",
    "title": "",
    "section": "Why study Linear Algebra?",
    "text": "Why study Linear Algebra?\n\n\nIt is the magic recipe behind everything computational\nIt is part of your everyday life! Some examples:\n\nPage Rank Algorithm (Google Search)\nVoice Assistants (Siri/Alexa)\nImage & Audio Editing Software\nChatGPT"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#why-study-linear-algebra-linear-system",
    "href": "pages/CNMLabSlides/Intro.html#why-study-linear-algebra-linear-system",
    "title": "",
    "section": "Why study Linear Algebra & Linear System?",
    "text": "Why study Linear Algebra & Linear System?\n\n\nIt is the magic recipe behind everything computational\nIt is part of your everyday life!\n\nSearch Algorithms\nVoice Assistants (Siri/Alexa)\nImage & Audio Editing Software\nChatGPT & All other Deep Neural Networks"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-example",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-example",
    "title": "",
    "section": "Image editing example",
    "text": "Image editing example"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra",
    "title": "",
    "section": "Image Editing using Linear Algebra",
    "text": "Image Editing using Linear Algebra\n\nusing Images, ImageIO, ImageMagick\nutd = joinpath(pwd(),\"utd.jpeg\") \nutdimg = load(utd)\nutdimg"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-1",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-1",
    "title": "",
    "section": "Image Editing using Linear Algebra",
    "text": "Image Editing using Linear Algebra\n\n\nFloat64.(Gray.(utdimg))\n\n334×500 Matrix{Float64}:\n 0.490196   0.490196   0.490196   …  0.443137    0.443137   0.443137\n 0.490196   0.490196   0.494118      0.447059    0.447059   0.447059\n 0.494118   0.494118   0.494118      0.447059    0.447059   0.447059\n 0.498039   0.498039   0.498039      0.45098     0.45098    0.45098\n 0.501961   0.501961   0.501961      0.454902    0.454902   0.454902\n 0.505882   0.505882   0.505882   …  0.458824    0.458824   0.458824\n 0.509804   0.509804   0.509804      0.458824    0.458824   0.458824\n 0.509804   0.509804   0.513725      0.462745    0.462745   0.462745\n 0.513725   0.513725   0.513725      0.462745    0.462745   0.462745\n 0.517647   0.517647   0.517647      0.462745    0.462745   0.462745\n 0.521569   0.521569   0.517647   …  0.466667    0.466667   0.466667\n 0.52549    0.52549    0.521569      0.466667    0.466667   0.466667\n 0.529412   0.529412   0.52549       0.470588    0.470588   0.470588\n ⋮                                ⋱                         \n 0.054902   0.0117647  0.0627451     0.423529    0.54902    0.243137\n 0.152941   0.0666667  0.0705882     0.466667    0.52549    0.478431\n 0.360784   0.196078   0.101961      0.141176    0.152941   0.203922\n 0.341176   0.164706   0.0745098  …  0.0235294   0.152941   0.27451\n 0.176471   0.0509804  0.027451      0.00392157  0.0745098  0.164706\n 0.133333   0.0313725  0.0313725     0.0627451   0.0235294  0.0941176\n 0.0588235  0.0862745  0.0156863     0.113725    0.0745098  0.0352941\n 0.423529   0.388235   0.0627451     0.184314    0.105882   0.00392157\n 0.333333   0.184314   0.0666667  …  0.243137    0.321569   0.105882\n 0.298039   0.34902    0.235294      0.164706    0.423529   0.541176\n 0.364706   0.235294   0.403922      0.180392    0.478431   0.321569\n 0.196078   0.27451    0.258824      0.141176    0.521569   0.470588"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-2",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-2",
    "title": "",
    "section": "Image Editing using Linear Algebra",
    "text": "Image Editing using Linear Algebra\n\n\n\nGray.(utdimg)\n\n\n\n\n\n\nGray.(utdimg) .+ 0.5"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-3",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-3",
    "title": "",
    "section": "Image Editing using Linear Algebra",
    "text": "Image Editing using Linear Algebra\n\n\n\nGray.(utdimg)\n\n\n\n\n\n\nGray.(utdimg) .- 0.5"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-4",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-4",
    "title": "",
    "section": "Image Editing using Linear Algebra",
    "text": "Image Editing using Linear Algebra\n\n\n\nGray.(utdimg)\n\n\n\n\n\n\nGray.(utdimg) .* 0.5"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-5",
    "href": "pages/CNMLabSlides/Intro.html#image-editing-using-linear-algebra-5",
    "title": "",
    "section": "Image Editing using Linear Algebra",
    "text": "Image Editing using Linear Algebra\n\n\n\nGray.(utdimg)\n\n\n\n\n\n\nGray.(utdimg)[1:5:end,1:5:end]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#building-bocks-of-linear-algebra",
    "href": "pages/CNMLabSlides/Intro.html#building-bocks-of-linear-algebra",
    "title": "",
    "section": "Building Bocks of Linear Algebra",
    "text": "Building Bocks of Linear Algebra\n\n\nScalars, c\n\ndenoted by italics lowercase character\n\n0D\n\n\n\nc = 2 \n\n2\n\n\n\nVectors, \\(\\mathbf{x}= [ x_1, x_2, x_3]\\)\n\ndenoted by boldface lowercase character\n\nelements of a vector are denoted by lower case characters with subscripts\nsubscripts denote the position of the elements in the vector\n\n1D\n\n\n\n# Row Vector \nx = [1,2,3] \n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n# Column Vector \nx = [1 2 3] \n\n1×3 Matrix{Int64}:\n 1  2  3"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#building-bocks-of-linear-algebra-1",
    "href": "pages/CNMLabSlides/Intro.html#building-bocks-of-linear-algebra-1",
    "title": "",
    "section": "Building Bocks of Linear Algebra",
    "text": "Building Bocks of Linear Algebra\n\n\nMatrices, \\[\\mathbf{A} = \\begin{bmatrix}\n  a_{11} & a_{12} & c_{13} \\\\\n  a_{21} & a_{22} & a_{23}\\\\\n  \\end{bmatrix}\\]\n\ndenoted by uppercase boldface character\n\nelements of a matrix are denoted by lowercase character with subscripts\nsubscript denotes the row number and column number of the element\n\n2D\n\n\n\nA = [21 12 99;\n    11 43 51] \n\n2×3 Matrix{Int64}:\n 21  12  99\n 11  43  51\n\n\n\nTensors,\n\nN dimensional, where \\(N > 2\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#building-blocks-of-linear-algebra",
    "href": "pages/CNMLabSlides/Intro.html#building-blocks-of-linear-algebra",
    "title": "",
    "section": "Building Blocks of Linear Algebra",
    "text": "Building Blocks of Linear Algebra\n\n\nScalars, c\n\ndenoted by italics lowercase character\n\n0D Array\n\n\n\nc = 2 \n\n2\n\n\n\nVectors, \\(\\mathbf{x}= [ x_1, x_2, x_3]\\)\n\ndenoted by boldface lowercase character\n\nelements of a vector are denoted by lower case characters with subscripts\nsubscripts denote the position of the elements in the vector\n\n1D Array\n\n\n\n# Column Vector \nx = [1,2,3] \n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n# Row Vector \nx = [1 2 3] \n\n1×3 Matrix{Int64}:\n 1  2  3"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#building-blocks-of-linear-algebra-1",
    "href": "pages/CNMLabSlides/Intro.html#building-blocks-of-linear-algebra-1",
    "title": "",
    "section": "Building Blocks of Linear Algebra",
    "text": "Building Blocks of Linear Algebra\n\n\nMatrices, \\[\\mathbf{A} = \\begin{bmatrix}\n  a_{11} & a_{12} & c_{13} \\\\\n  a_{21} & a_{22} & a_{23}\\\\\n  \\end{bmatrix}\\]\n\ndenoted by uppercase boldface character\n\nelements of a matrix are denoted by lowercase character with subscripts\nsubscript denotes the row number and column number of the element\n\n2D Array\n\n\n\nA = [21 12 99;\n    11 43 51] \n\n2×3 Matrix{Int64}:\n 21  12  99\n 11  43  51\n\n\n\nTensors,\n\nN dimensional Array, where \\(N > 2\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vectors-are-points-in-space",
    "href": "pages/CNMLabSlides/Intro.html#vectors-are-points-in-space",
    "title": "",
    "section": "Vectors are points in space",
    "text": "Vectors are points in space"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vectors-are-points-in-space-1",
    "href": "pages/CNMLabSlides/Intro.html#vectors-are-points-in-space-1",
    "title": "",
    "section": "Vectors are points in space",
    "text": "Vectors are points in space"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#scalar-vector-multiplication",
    "href": "pages/CNMLabSlides/Intro.html#scalar-vector-multiplication",
    "title": "",
    "section": "Scalar Vector Multiplication",
    "text": "Scalar Vector Multiplication\n\n\n\\(\\mathbf{x} = [4,7]\\)\n\n\n\n\n\n\n\\(\\mathbf{x} \\times 0.2 = [4*0.2, 7*0.2] = [0.8, 1.4]\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#scalar-vector-multiplication-1",
    "href": "pages/CNMLabSlides/Intro.html#scalar-vector-multiplication-1",
    "title": "",
    "section": "Scalar Vector Multiplication",
    "text": "Scalar Vector Multiplication\n\n\n\\(\\mathbf{x} = [4,7]\\)\n\n\n\n\n\n\n\\(\\mathbf{x} \\times -1 = [4* -1, 7* -1] = [-4, -7]\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-addition",
    "href": "pages/CNMLabSlides/Intro.html#vector-addition",
    "title": "",
    "section": "Vector Addition",
    "text": "Vector Addition\n\n\n\\(\\mathbf{x} = [1,2]\\)\n\n\n\n\n\n\n\\(\\mathbf{y} = [3,1]\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-addition-1",
    "href": "pages/CNMLabSlides/Intro.html#vector-addition-1",
    "title": "",
    "section": "Vector Addition",
    "text": "Vector Addition\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{x} = [1,2]\\)\n\\(\\mathbf{y} = [3,1]\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-addition-2",
    "href": "pages/CNMLabSlides/Intro.html#vector-addition-2",
    "title": "",
    "section": "Vector Addition",
    "text": "Vector Addition\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{x} = [1,2]\\)\n\\(\\mathbf{y} = [3,1]\\)\n\\(\\mathbf{x} + \\mathbf{y} = \\\\ [1,2] + [3,1] = \\\\ [1+3, 2+1] = \\\\ [4,3]\\)\nIn Julia:\n\n\nx =[1,2]\ny = [3,1]\nx+y\n\n2-element Vector{Int64}:\n 4\n 3"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-multiplication",
    "href": "pages/CNMLabSlides/Intro.html#vector-multiplication",
    "title": "",
    "section": "Vector Multiplication",
    "text": "Vector Multiplication\nThere are several ways to multiply two vectors:\n\nDot Product (Inner Product), \\(\\qquad \\mathbf{a} \\cdot \\mathbf{b}\\)\nOuter Product \\(\\qquad \\qquad \\quad \\mathbf{a} \\otimes \\mathbf{b}\\)\nCross Product \\(\\quad \\quad \\quad \\qquad \\mathbf{a} \\times \\mathbf{b}\\)\nHadamard Product \\(\\quad \\quad \\quad \\mathbf{a} \\odot \\mathbf{b}\\)\nKronecker Product \\(\\quad \\quad \\quad \\mathbf{a} \\otimes \\mathbf{b}\\)\n\nNote: These are not the same! Each method gives you a different output."
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-dot-product",
    "href": "pages/CNMLabSlides/Intro.html#vector-dot-product",
    "title": "",
    "section": "Vector Dot Product",
    "text": "Vector Dot Product\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\begin{bmatrix}  a_{1} \\\\ \\vdots \\\\ a_{n}  \\end{bmatrix}\n\\boldsymbol{\\bullet}\n\\begin{bmatrix} b_{1} \\\\ \\vdots \\\\ b_{n} \\end{bmatrix}\n= \\sum_{i=1}^{n} a_i b_i\n\\]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#t",
    "href": "pages/CNMLabSlides/Intro.html#t",
    "title": "",
    "section": "t",
    "text": "t\nConsider two vectors, \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\end{bmatrix}\\), \\(\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#d",
    "href": "pages/CNMLabSlides/Intro.html#d",
    "title": "",
    "section": "d",
    "text": "d\n\n::: ::: {.column width=“50%”} ::::"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-dot-product-1",
    "href": "pages/CNMLabSlides/Intro.html#vector-dot-product-1",
    "title": "",
    "section": "Vector Dot Product",
    "text": "Vector Dot Product\nConsider two vectors, \\(\\mathbf{v} = \\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\end{bmatrix}\\), \\(\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}\\)\n\\[\n\\mathbf{v} \\cdot \\mathbf{w} = (3 \\times 1) + (-1 \\times 2) + (2 \\times 1) \\\\\n= 3\n\\]\n\nusing LinearAlgebra\nv = [3 -1 2]\nw = [1 2 1]\ndot(v,w)\n\n3\n\n\nAlternatively,\n\nv * w'\n\n1×1 Matrix{Int64}:\n 3"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#vector-magnitude",
    "href": "pages/CNMLabSlides/Intro.html#vector-magnitude",
    "title": "",
    "section": "Vector Magnitude",
    "text": "Vector Magnitude\n\nSquareroot of the inner product of a vector with itself gives the magnitude of that vector"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#section",
    "href": "pages/CNMLabSlides/Intro.html#section",
    "title": "",
    "section": "",
    "text": "\\(\\mathbf{x} = [1,2]\\)\n\\(\\mathbf{y} = [3,1]\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#section-1",
    "href": "pages/CNMLabSlides/Intro.html#section-1",
    "title": "",
    "section": "",
    "text": "\\(\\mathbf{x} = [1,2]\\)\n\\(\\mathbf{y} = [3,1]\\)\n\\(\\mathbf{x} + \\mathbf{y} = \\\\ [1,2] + [3,1] = \\\\ [1+3, 2+1] = \\\\ [4,3]\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#distance-between-vectors",
    "href": "pages/CNMLabSlides/Intro.html#distance-between-vectors",
    "title": "",
    "section": "Distance between vectors",
    "text": "Distance between vectors\nL1 Norm / Manhattan Distance\n\n\n\n\n\n\n\n\n\n\\(|\\mathbf{u} -\\mathbf{v}|_1 = \\\\ |3-1| + |1-2| = \\\\ |2| + |-1| \\\\ = 3\\)\n\n\nusing LinearAlgebra\nu = [1,2]\nv = [3,1]\nnorm(u-v,1)\n\n3.0"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#s",
    "href": "pages/CNMLabSlides/Intro.html#s",
    "title": "",
    "section": "s",
    "text": "s"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#distance-between-vectors-1",
    "href": "pages/CNMLabSlides/Intro.html#distance-between-vectors-1",
    "title": "",
    "section": "Distance between vectors",
    "text": "Distance between vectors\nL2 Norm / Euclidean Distance\n\n\n\n\n\n\n\n\n\n\\(|\\mathbf{u} -\\mathbf{v}|_2 = \\\\ \\sqrt{(3-1)^2 + (1-2)^2} = \\\\ \\sqrt{(2)^2 + (-1)^2} =\\\\ \\sqrt{4+1} = \\sqrt{5} = 2.2361\\)\n\n\nusing LinearAlgebra\nu = [1,2]\nv = [3,1]\nnorm(u-v,2)\n\n2.23606797749979"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#distance-between-vectors-2",
    "href": "pages/CNMLabSlides/Intro.html#distance-between-vectors-2",
    "title": "",
    "section": "Distance between vectors",
    "text": "Distance between vectors\nAngular Separation\n\n\n\n\n\n\n\n\n\n\\[\\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{||\\mathbf{u}|| ||\\mathbf{v}||} = \\] \\(\\frac{(1 \\times 3) + (2 \\times 1)}{\\sqrt{(1)^1+(2)^2} \\times \\sqrt{(3)^2+(1)^2}} = \\frac{1}{\\sqrt{2}}\\)\n\\(\\theta = \\cos^{-1} (\\frac{1}{\\sqrt{2}}) = \\\\ 45^{\\circ}\\)\n\n\nusing LinearAlgebra\nu = [1,2]\nv = [3,1]\nunorm = norm(u,2)\nvnorm = norm(v,2)\nacosd(dot(u,v) / (unorm*vnorm))\n\n45.00000000000001"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#distance-between-vectors-3",
    "href": "pages/CNMLabSlides/Intro.html#distance-between-vectors-3",
    "title": "",
    "section": "Distance between vectors",
    "text": "Distance between vectors\nAngular Separation is a similarity measure\n\n\n\n\n\n\n\n\n\nusing LinearAlgebra\nu = [1,2]\nv = [1.5,3]\nunorm = norm(u,2)\nvnorm = norm(v,2)\nacosd(dot(u,v) / (unorm*vnorm))\n\n8.537736462515939e-7"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#linear-neuron-model",
    "href": "pages/CNMLabSlides/Intro.html#linear-neuron-model",
    "title": "",
    "section": "Linear Neuron Model",
    "text": "Linear Neuron Model\n\n\n\n\n\nWhat is the output of this neuron model?"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-operation",
    "href": "pages/CNMLabSlides/Intro.html#matrix-operation",
    "title": "",
    "section": "Matrix Operation",
    "text": "Matrix Operation\n\n\nAddition\n\nM = [2 6;8 4]\n\n2×2 Matrix{Int64}:\n 2  6\n 8  4\n\n\n\nN = [1 0;-1 3]\n\n2×2 Matrix{Int64}:\n  1  0\n -1  3\n\n\n\nM + N\n\n2×2 Matrix{Int64}:\n 3  6\n 7  7\n\n\n\nMultiplication by Scalar\n\n3 * M\n\n2×2 Matrix{Int64}:\n  6  18\n 24  12\n\n\nMatrix Transpose\n\ntranspose(M)\n\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 2  8\n 6  4"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-operations",
    "href": "pages/CNMLabSlides/Intro.html#matrix-operations",
    "title": "",
    "section": "Matrix Operations",
    "text": "Matrix Operations\n\n\nAddition\n\nM = [2 6;8 4]\n\n2×2 Matrix{Int64}:\n 2  6\n 8  4\n\n\n\nN = [1 0;-1 3]\n\n2×2 Matrix{Int64}:\n  1  0\n -1  3\n\n\n\nM + N\n\n2×2 Matrix{Int64}:\n 3  6\n 7  7\n\n\n\nMultiplication by Scalar\n\n3 * M\n\n2×2 Matrix{Int64}:\n  6  18\n 24  12\n\n\nMatrix Transpose\n\ntranspose(M)\n\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 2  8\n 6  4"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#l-d",
    "href": "pages/CNMLabSlides/Intro.html#l-d",
    "title": "",
    "section": "l d",
    "text": "l d"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication",
    "href": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication",
    "title": "",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\nAs Linear Transformation\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{u} = \\begin{bmatrix}4 \\\\1 \\end{bmatrix}\\)\nMultiply \\(\\mathbf{u}\\) by \\(\\mathbf{A} = \\begin{bmatrix}1 & -3 \\\\1 & -1 \\end{bmatrix}\\)"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-1",
    "href": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-1",
    "title": "",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\nAs Linear Transformation"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-2",
    "href": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-2",
    "title": "",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\nAs Linear Transformation\n\n\n\\(\\begin{bmatrix}1 & -3 \\\\1 & -1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} =\\) \\[ 4 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} -3 \\\\ - 1\\end{bmatrix} =\\]\n\\[\\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\\]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-3",
    "href": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-3",
    "title": "",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\n\nAs Linear Transformation (Coordinate Transformation)\n\n\n\n\n\n\nblue represents the transformed coordinates"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-4",
    "href": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-4",
    "title": "",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\nWhat is the value of \\(\\mathbf{u}\\)? \\[\\mathbf{u} = \\begin{bmatrix} 3 & 4 & 5 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 2\\end{bmatrix}\\]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-5",
    "href": "pages/CNMLabSlides/Intro.html#matrix-vector-multiplication-5",
    "title": "",
    "section": "Matrix-Vector Multiplication",
    "text": "Matrix-Vector Multiplication\nWhat is the value of \\(\\mathbf{u}\\)? \\[\\mathbf{u} = \\begin{bmatrix} 3 & 4 & 5 \\\\ 1 & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 2\\end{bmatrix}\\]\n\\(= 1 \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} + 0 \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix} + 2 \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\\)\n\\(= \\begin{bmatrix} 13 \\\\ 3 \\end{bmatrix}\\)\n\nw = [3 4 5; 1 0 1]\nv= [1,0,2]\nw*v\n\n2-element Vector{Int64}:\n 13\n  3"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#connection-weight-matrix",
    "href": "pages/CNMLabSlides/Intro.html#connection-weight-matrix",
    "title": "",
    "section": "Connection Weight Matrix",
    "text": "Connection Weight Matrix\n\n\n\n\n\\[\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13} & w_{14} \\\\\nw_{21} & w_{22} & w_{23} & w_{24}\\\\\nw_{31} & w_{32} & w_{33} & w_{34}\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{r} = \\mathbf{W} \\mathbf{x}\n\\]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#connection-weight-matrix-1",
    "href": "pages/CNMLabSlides/Intro.html#connection-weight-matrix-1",
    "title": "",
    "section": "Connection Weight Matrix",
    "text": "Connection Weight Matrix\n\n\n\n\n\\[\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13} & 0 \\\\\n0 & 0 & 0 & w_{24}\\\\\n0 & 0 & w_{33} & w_{34}\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{r} = \\mathbf{W} \\mathbf{x}\n\\]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#connection-weight-matrix-2",
    "href": "pages/CNMLabSlides/Intro.html#connection-weight-matrix-2",
    "title": "",
    "section": "Connection Weight Matrix",
    "text": "Connection Weight Matrix\n\n\n\n\n\\[\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13} & 0 \\\\\n0 & 0 & 0 & w_{24}\\\\\n0 & 0 & w_{33} & w_{34}\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{r} = \\mathbf{W} \\mathbf{x}\n\\]\n\\[\n\\mathbf{y} = \\mathbf{H} \\mathbf{r}\n\\]"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-matrix-multiplication",
    "href": "pages/CNMLabSlides/Intro.html#matrix-matrix-multiplication",
    "title": "",
    "section": "Matrix-Matrix Multiplication",
    "text": "Matrix-Matrix Multiplication\n\n\\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 1 \\\\ 1 & -1 \\\\ \\end{bmatrix} \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\\\ \\end{bmatrix} =\\)\n\\[\\begin{bmatrix} \\begin{bmatrix} 1 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 \\\\ 1\\end{bmatrix} &\n\\begin{bmatrix} 1 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\\\\n\\begin{bmatrix} 3 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 \\\\ 1\\end{bmatrix} &\n\\begin{bmatrix} 3 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\\\\n  \\begin{bmatrix} 1 & -1 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 \\\\ 1\\end{bmatrix} &\n\\begin{bmatrix} 1 & -1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix} \\\\\n\\end{bmatrix} = \\]\n\\[\\begin{bmatrix} 5 & 5 \\\\ 10 & 5 \\\\ 2 & - 1 \\end{bmatrix}\\]\n\nA = [1 2; 3 1;1 -1]\nB = [3 1; 1 2]\nA * B\n\n3×2 Matrix{Int64}:\n  5   5\n 10   5\n  2  -1"
  },
  {
    "objectID": "pages/CNMLabSlides/Intro.html#matrix-matrix-multiplication-1",
    "href": "pages/CNMLabSlides/Intro.html#matrix-matrix-multiplication-1",
    "title": "",
    "section": "Matrix-Matrix Multiplication",
    "text": "Matrix-Matrix Multiplication\n\nA = [1 2; 3 1;1 -1]\nB = [3 1; 1 2]\nA * B\n\n3×2 Matrix{Int64}:\n  5   5\n 10   5\n  2  -1"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices",
    "title": "",
    "section": "Recall: Images are Matrices",
    "text": "Recall: Images are Matrices"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-1",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-1",
    "title": "",
    "section": "Recall: Images are Matrices",
    "text": "Recall: Images are Matrices\n\n\n\n\n\n\n\n\n\n(334, 500)\n\n\n\n\nutdimg[5,5]\n\n\n\n\n\nutdimg[105,305]\n\n\n\n\n\nutdimg[334,500]"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-2",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-2",
    "title": "",
    "section": "Recall: Images are Matrices",
    "text": "Recall: Images are Matrices\n\n\n\nr = RGB.(red.(utdimg), zeros(m,n), zeros(m,n))\n\n\n\n\n\n\ng = RGB.(zeros(m,n), green.(utdimg), zeros(m,n))\n\n\n\n\n\n\nb = RGB.(zeros(m,n), zeros(m,n), blue.(utdimg))\n\n\n\n\n\n\n\n\n\n\n\nr .+ b .+ g"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-3",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-3",
    "title": "",
    "section": "Recall: Images are Matrices",
    "text": "Recall: Images are Matrices\n\n\n\nutdbnw = Gray.(utdimg)\n\n\n\n\n\n\n(334, 500)\n\n\n\nutdbnw[100:170, 260:490]\n\n\n\n\n\n\nutdbnw[5,5]\n\n\n\n\n\nutdbnw[105,305]\n\n\n\n\n\nutdbnw[334,500]"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-4",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#recall-images-are-matrices-4",
    "title": "",
    "section": "Recall: Images are Matrices",
    "text": "Recall: Images are Matrices\n\n\n\nutdbnw = Gray.(utdimg)\n\n\n\n\n\n\n(334, 500)\n\n\n\n\n\nvec(Float64.(utdbnw))\n\n167000-element Vector{Float64}:\n 0.49019607843137253\n 0.49019607843137253\n 0.49411764705882355\n 0.4980392156862745\n 0.5019607843137255\n 0.5058823529411764\n 0.5098039215686274\n 0.5098039215686274\n 0.5137254901960784\n 0.5176470588235295\n 0.5215686274509804\n 0.5254901960784314\n 0.5294117647058824\n ⋮\n 0.24313725490196078\n 0.47843137254901963\n 0.20392156862745098\n 0.27450980392156865\n 0.16470588235294117\n 0.09411764705882353\n 0.03529411764705882\n 0.00392156862745098\n 0.10588235294117647\n 0.5411764705882353\n 0.3215686274509804\n 0.47058823529411764"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#image-compression-using-svd",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#image-compression-using-svd",
    "title": "",
    "section": "Image Compression using SVD",
    "text": "Image Compression using SVD\n\nusing LinearAlgebra \nU,S,V = svd(Float64.(Gray.(utdbnw)))\nRGB.(U*diagm(S)*V')"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#image-compression-using-svd-1",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#image-compression-using-svd-1",
    "title": "",
    "section": "Image Compression using SVD",
    "text": "Image Compression using SVD\n\nfunction compressimg(n)\n    RGB.(U[:,1:n]*diagm(S[1:n])*V[:,1:n]')\nend\n\ncompressimg (generic function with 1 method)\n\n\n\ncompressimg(20)"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#whats-the-optimal-number-of-features",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#whats-the-optimal-number-of-features",
    "title": "",
    "section": "What’s the optimal number of features ?",
    "text": "What’s the optimal number of features ?"
  },
  {
    "objectID": "pages/CNMLabSlides/EigenAnalysis.html#whats-the-optimal-number-of-features-1",
    "href": "pages/CNMLabSlides/EigenAnalysis.html#whats-the-optimal-number-of-features-1",
    "title": "",
    "section": "What’s the optimal number of features ?",
    "text": "What’s the optimal number of features ?\n\n\n\n\n\n\n\n\n\ncompressimg(10)\n\n\n\n\n\n\n\n\ncompressimg(20)\n\n\n\n\n\ncompressimg(30)"
  },
  {
    "objectID": "pages/CNMLabSlides/Lab1/lab1.html",
    "href": "pages/CNMLabSlides/Lab1/lab1.html",
    "title": "Lab 1: Hello Julia! ",
    "section": "",
    "text": "Train you with the necessary tools and skills you need to complete your modeling project\n\n\n\nIdentify an engineering or scientific problem in Artificial Intelligence, Psychology, or Cognitive Neuroscience.\nIdentify & assess the quality of the data required to empirically investigate the problem.\nDevelop a 1 page project proposal abstract.\nUse the machine learning skills acquired through the lab and theory to investigate the problem.\n\nRecode the data using unsupervised learning (if necessary).\nImplement a simple logistic regression model, shallow neural network, and a deep neural network model.\nEvaluate the performance of your model (Confusion Matrix).\nDo 2-fold cross-validation.\nCompare the results of competing models.\n\nPresent the results of your study as a 15-minute talk.\nWrite and submit a ~10 page project report in APA Journal Article format.\n\n\n\n\n\nYou fill out a project interest survey\n\nArtificial Intelligence Project Examples:\n\nEmail Spam Filter, Speech Recognition System, Twitter Hate Speech Detection\n\nPsychology Project Examples:\n\nHuman Emotion Classification using Eye-Feature, Psychopathy Diagnosis\n\nCognitive Neuroscience Project Examples:\n\nAlzheimer’s detection using fMRI, Analysis and Modeling of Neural Ensemble Rehearsal During Sleep, Tumor detection using MRI, Autism diagnosis using fMRI\n\n\nYou form groups by yourself or we help you find project groups\n\n\n\n\n\n\n\nKaggle: For all sorts of datasets.\nUCI Machine Learning Repository: For all sorts of datasets.\nUCLA Library: For Psychological datasets.\nCMU Datashop: For Educational & Psychological datasets.\nOpenNeuro: For Neuroscience datasets.\nCRCNS: For Neuroscience datasets.\nRDatasets: A collection of common datasets used in all Statistical/Machine Learning textbooks.\nGoogle Dataset Search\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Source (Free to Use, No license required to do commercial project in Julia)\nLess verbose (compared to R & Python)\nBest-in-class interoperability (with R, Python & C)\nReally fast when implementing computationally complex algorithms\nGood software engineering principles instantiated in the language design\nUsed widely in High Performance Scientific Computing\n\n\n\n\nRelatively young community –> fewer packages compared to R or Python (RCall.jl & PythonCall.jl to the rescue)\nPackage installation can take some time (continually improving with each update)\n\n\n\n\n\n\nInstalling Julia\nChange language type in a new file\nKnow your IDE\nJulia REPL Modes\nInstalling Packages\nCreating a Julia Script\nProject Environments"
  }
]